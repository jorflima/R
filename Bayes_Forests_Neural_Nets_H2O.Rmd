---
title: "H2o - Random Forest"
author: "Jorge Lima"
output: html_document
---


# End Stage Renal Disease - H2o

On the previous exercise (weeks 5 & 6) I used a 20% sample of my data because of memory allocation problems. In this final project I will try to use the full dataset named "NIS_imputed" to fit three models: Naive Bayes, Random Forest and Deep Learning. I will use the H2o library / workframe.

This project will be more straightfoward because much of the discussion and all the data wrangling was made in the previous project.


```{r}
library(h2o)

## Creating an H2O cloud, using all cores and allocating 8 Gigabytes to memory 
h2o.init(nthreads=-1, max_mem_size = "8G")

## Removing any previous clusters
h2o.removeAll() 
```

Loading my dataset "NIS_imputed" generated on the previous exercise:

```{r}
df <- h2o.importFile(path = normalizePath("/home/jorge/Documents/Dartmouth_Fall_2019/Independent_Study_Prof_Ramesh/Renal_Class_HCUP/NIS_imputed.csv"))
```

Splitting my data into train (60%), validation (20%) and test (20%) sets:

```{r}
splits <- h2o.splitFrame( df, c(0.6,0.2), seed=42)    

#It calculates the third set size by doing: 1 - (0.6 + 0.2) = 1- 0.8 = 0.2

train <- h2o.assign(splits[[1]], "train.hex") #First split is train, second is validation, last one is test   
valid <- h2o.assign(splits[[2]], "valid.hex") 
test <- h2o.assign(splits[[3]], "test.hex")     
```

#Taking a look at the data:
```{r}
#This is the same as head() in base R
#train[1:5,]
```

### Fitting a Niave Bayes model using laplace smoothing:

```{r echo = T, results = 'hide'}
bayes_m <- h2o.naiveBayes(x = 2:11, y=12, train, balance_classes = F, laplace = 3, seed = 42)
```


```{r}
h2o.performance(bayes_m, test)
```

My overall Error was 0.613 for the Test set which gives me an accuracy of just 38.7% which was lower than the previous exercise using the e1071 package.



### Fitting a Random Forest model:

H2o handled the bigger amount of data much better than base R with the randomForest package. This is the reason I am able know to experiment further and use cartesian grid search with varying tree depths (5, 7 and 10) and number of trees (base of 50, then 100 and finally 200) so find the best fit:

```{r echo = T, results = 'hide'}
param = list(ntrees = c(50, 100, 200), max_depth = c(5, 7, 10), seed = 42)
grid <- h2o.grid("randomForest", grid_id = "grid1", x=2:11, y=12, training_frame = train, validation_frame = valid, hyper_params = param, balance_classes = TRUE)
```

Getting the metrics:
```{r}
summary(grid)
```
This is sorted by logloss, I can change the metric. Lets use accuracy instead so it is more easily comparable to the models in the previous exercise:

```{r}
rf_sorted <- h2o.getGrid(grid_id = "grid1", sort_by = "accuracy", decreasing = T)

print(rf_sorted)
```

I will now select the best model from an accuracy point of view and get its summary statistics:

```{r}
best_model <- h2o.getModel(rf_sorted@model_ids[[1]])
summary(best_model)
```

So the model chosen had a max depth of 10, used 100 trees and had an accuracy of approximately 42.85% which is currently the best model. The second best was random forest using base parameters from the last exercise. This was considerably better than the Naive Bayes approach. Variables that were important are Age and Race which was consistent with the previous exercise. The news here is that wscore_ahrq was placed second and was not valued high before. 



Now we are going to fit the model on my Test set and see how well it performs:

```{r}
h2o.performance(best_model, test)
```

The performance was close to the train set with an accuracy of 42.79%


We can get the same information by taking the mean of the predictions for each class:
```{r}
best_model_predictions<-h2o.predict(object = best_model, newdata = test)

mean(best_model_predictions$predict == test$Stage)
```
As expected the same rounded result (42.79%)



### Fitting a deep learning model:

The last step on this project is to fit a Deep Learning model.

Here I am excluding the ID column from the dataset and naming the predictor variables:
```{r}
df <- df[-1]

response <- "Stage"
predictors <- setdiff(names(df), response)
predictors
```

The reasoning the choice of variables and their exploratory graphs can be found on the previous exercise.

Fitting my Deep Learning model using the default parameters (2 hidden layers with 200 neuros each):

```{r echo = T, results = 'hide'}
m1 <- h2o.deeplearning(
  model_id="dl_model_first", 
  training_frame=train, 
  validation_frame=valid, x=predictors, y=response, epochs=1, variable_importances=T, seed = 42)
```

```{r}
summary(m1)
```

I got an overall accuracy which is a little worse than what we got from the random forest using grid search.


Importance of variables:

```{r}
head(as.data.frame(h2o.varimp(m1)))
```

The importance of variables changed here. AGE is still important, but lenght of stay (LOS) and costs (TOTCHG) climbed up the ladder.


Applying the model in my Test set:
```{r}
m1_predictions<-h2o.predict(object = m1, newdata = test, seed = 42)

mean(m1_predictions$predict == test$Stage)
```

I got a smilar accuracy using my test set.



I will now try to find the best model by using grid search with different hidden layer sizes and neuron numbers, learning rates, imput layer dropout ratio (to improve generalization) and learning rate step decay (rate_annealing).

I will train the model for 20 epochs and stop when my missclassification do not improve by more than 1% after 2 scoring events.

```{r echo = T, results = 'hide'}
hyper_params <- list(
  hidden=list(c(32,32,32),c(64,64), c(128, 128, 128)), input_dropout_ratio=c(0,0.1), rate=c(0.01,0.05),
  rate_annealing=c(1e-8,1e-7,1e-6))


grid_dl <- h2o.grid(algorithm="deeplearning", grid_id="dl_grid", training_frame=train, validation_frame=valid, x=predictors, y=response,
  epochs=20, stopping_metric="misclassification", stopping_tolerance=1e-2, stopping_rounds=2, score_validation_samples=10000,
  score_duty_cycle=0.025, adaptive_rate=F,  momentum_start=0.5, momentum_stable=0.9, momentum_ramp=1e7, l1=1e-5,l2=1e-5,
  activation=c("Rectifier"), max_w2=10, seed= 42, hyper_params=hyper_params)

```

Models in the grid:
```{r}
grid_dl
```

Sorting by decreasing accuracy:
```{r}
h2o.getGrid("dl_grid", sort_by="accuracy", decreasing= T)
```

Getting the stats from the best model:
```{r}
grid_dl@summary_table[1,]
best_model2 <- h2o.getModel(grid_dl@model_ids[[1]])
summary(best_model2)
plot(best_model2)
```

I managed to improve my accuracy a little using grid search. Costs, Age, Length of Stay and Charlson index were the most important variables. 

Let's see now how it fares using the model on my test data:

```{r}
h2o.performance(best_model2, test)
```


```{r}
best_model_predictions2<-h2o.predict(object = best_model2, newdata = test, seed =42)

mean(best_model_predictions2$predict == test$Stage)
```

This is the best model with a slightly higher accuracy than the result using grid search with random forest. 

Generally speaking though this dataset was not very informative regarding classification of CKD stages. As seen before in the past exercise only Stage 3 was reliably categorized by the algorithms.



Additional metrics used for exploring H2o library (not commented on):
```{r}
#print(best_model2@allparameters)
#print(h2o.performance(best_model2, valid=T))
#print(h2o.logloss(best_model2, valid=T))
```

Shtting rhe H2o cluster down:

```{r}
h2o.shutdown(prompt=FALSE)
```
